---
title: "Understanding Secret Use of Large Language Models"
summary: "Zhiping Zhang, __Chenxinran Shen__, Bingsheng Yao, Dakuo Wang, Tianshi Li<br>  _Proceedings of the ACM on Human-Computer Interaction, 5(CSCW) (to appear at CSCW 2024) 2024_"
tags:
  - HAI
date: '2023-12-28T00:00:00Z'

# Optional external URL for project (replaces project detail page).
external_link: ''

image:
  caption: CHI 2021
  focal_point: Smart

# links:
#   - icon: twitter
#     icon_pack: fab
#     name: Follow
#     url: https://twitter.com/georgecushen
url_code: ''
url_pdf: 'https://dl.acm.org/doi/pdf/10.1145/3711061'
url_slides: ''
url_video: ''

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
---
The advancements of Large Language Models (LLMs) have decentralized the responsibility for the transparency
of AI usage. Specifically, LLM users are now encouraged or required to disclose the use of LLM-generated
content for varied types of real-world tasks. However, an emerging phenomenon, users’ secret use of LLM,
raises challenges in ensuring end users adhere to the transparency requirement. Our study used mixed-methods
with an exploratory survey (125 real-world secret use cases reported) and a controlled experiment among
300 users to investigate the contexts and causes behind the secret use of LLMs. We found that such secretive
behavior is often triggered by certain tasks, transcending demographic and personality differences among
users. Task types were found to affect users’ intentions to use secretive behavior, primarily through influencing
perceived external judgment regarding LLM usage. Our results yield important insights for future work on
designing interventions to encourage more transparent disclosure of the use of LLMs or other AI technologies